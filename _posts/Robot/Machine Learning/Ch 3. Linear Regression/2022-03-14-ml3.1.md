---
title:  "3.1 Linear Basis Function Model"
excerpt: 

categories:
  - Robot
tags:
  - [Machine Learning]

toc: false
toc_sticky: false
 
date: 2022-03-14
last_modified_at: 2022-03-14

use_math: true
published: true
---

<br>

***
### 3.1.1 Linear Basis Function Model

가장 단순한 형태의 linear regression model은 input variable들의 linear combination을 바탕으로 한 model.

$$
y(\textbf{x}, \textbf{w}) = w_0 + w_1x + w_2x_2 + \dots + w_Dx_D
\tag{ $ 1 $ }
$$

<p align="center"><img src="/assets/image/machine_learning/prmlfigs-png/Figure3.1a.png" width="" height="" title="" alt=""><br/></p>

<br>

여기서 $\textbf{x} = (x_0, x_1, \dots w_D)^T$. Linear regression model의 가장 중요한 성질은 이 model이 parameter $w_0, \dots w_D$의 linear function이라는 것.

그런데, 이 model은 input variable $x_i$의 linear function이기도 하다. 이러한 model에는 약간 문제가 생길 수 있어서, 다음처럼 input variable에 대한 fixed nonlinear function들의 linear combination을 사용할 수 있다.

$$
y(\textbf{x}, \textbf{w}) = w_0 + \sum_{j=1}^{M-1}w_j \phi_j(\textbf{x})
\tag{ $ 2 $ }
$$

여기서 $\phi_j(\textbf{x})$가 바로 **<span style="color:red">basis function</span>**

Parameter $w_0$는 data에 고정된 offset을 표현. 따라서 이를 **<span style="color:red">bias parameter</span>**라고 부르기도 한다.

편의를 위해 추가적인 *pseudo-'basis function'* $\phi_0(\textbf{x})=1$을 정의하자. 그러면,

$$
y(\textbf{x}, \textbf{w}) = \sum_{j=0}^{M-1}w_j \phi_j(\textbf{x}) = \textbf{w}^T \phi(\textbf{x})
\tag{ $ 3 $ }
$$

<br>

***
### 3.1.2 Example of Basis Function

여러 함수들이 basis function으로 사용될 수 있다. 예를 들어,

$$
\phi_j(x) = \exp \left\{- \frac{(x- \mu_j)^2}{2 s^2}\right\}
$$

<p align="center"><img src="/assets/image/machine_learning/prmlfigs-png/Figure3.1b.png" width="" height="" title="" alt=""><br/></p>

여기서 $\mu_j$는 input space에서 basis function의 위치를 결정. Parameter $s$는 공간적 크기를 결정. 위 함수는 보통 **'Gaussian'** basis function이라고 한다.

<br>

또다른 예시는 **sigmoid** basis function.

$$
\phi_j(x) = \sigma \left(\frac{x- \mu_j}{s}\right)
$$

<p align="center"><img src="/assets/image/machine_learning/prmlfigs-png/Figure3.1c.png" width="" height="" title="" alt=""><br/></p>

여기서 $\sigma(a)$는 다음처럼 정의되는 **logistic sigmoid** function.

$$
\sigma(a) = \frac{1}{1+\exp (-a)}
$$

<br>

***
### 3.1.3 Maximum Likelihood

Target variable $t$는 deterministic function $y(\textbf{x}, \textbf{w})$와 Gaussian noise의 합으로 주어진다고 가정하자.

$$
t=y(\textbf{x}, \textbf{w})+\epsilon
$$

여기서 $\epsilon$은 $0$을 평균으로, $\beta$를 precision(variance의 역)으로 가지는 Gaussian random variable. 따라서 다음과 같이 적을 수 있다.

$$
p(t|\textbf{x}, \textbf{w}, \beta) = \mathcal{N}(t|y(\textbf{x}, \textbf{w}), \beta^{-1})
$$

Gaussian distribution이므로, conditional mean은

$$
\mathbb{E}[t|\textbf{x}] = \int tp(t|\textbf{x})dt = y(\textbf{x}, \textbf{w})
$$

<br>

Input dataset $\textbf{X} = \\{\textbf{x}_1, \dots, \textbf{x}_N\\}$과 이에 해당하는 target variable $\mathsf{t} = \\{t_1, \dots, t_N\\}^T$를 고려. 식 $(3)$을 이용하면 likelihood finction을 얻는다. 이때 data point는 i.i.d<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>로 가정.

$$
p(\mathsf{t}|\textbf{X}, \textbf{w}, \beta) = \prod_{n=1}^{N} \mathcal{N}(t_n|\textbf{w}^T \phi(\textbf{x}_n), \beta^{-1})
$$

<br>

Input variable의 분포는 modeling하지 않을 것이다. 따라서 $\textbf{x}$는 언제나 conditional variable set 안에 포함되어 있을 것. 그러니까 이제부터는 $\textbf{x}$를 빼고 $p(\mathsf{t}\textbf{w}, \beta)$로 쓸 것이다.

Likelihood function에 대해 로그를 취하면,

$$
\begin{align*}
\ln p(\mathsf{t}|\textbf{w}, \beta) &= \ln \sum_{n=1}^{N} \mathcal{N}(t_n|\textbf{w}^T \phi(\textbf{x}_n), \beta^{-1}) \\
&= \frac{N}{2} \ln \beta - \frac{N}{2} \ln (2 \pi) - \beta E_D(\textbf{W})
\end{align*}
\tag{ $ 4 $ }
$$

여기서 error function은 sum-of-square로,

$$
E_D(\textbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \{t_n - \textbf{w}^T \phi(\textbf{x}_n)\}^2 
$$

<br>

이제 maximum likelihood method를 적용. 식 $(4)$는 $\textbf{w}$와 $\beta$에 대한 함수이므로, 이들에 대해 극대화시키도록 하자.

Sec 1.2에서 보았듯, Gaussian noise distribution에 대해 likelihood function을 최대화하는 것과 sum-of-square error function $E_D(\textbf{w})$를 최소화하는 것은 같다. 그러므로 $(4)$의 gradient는

$$
\nabla_{\textbf{w}} \ln p(\mathsf{t}|\textbf{w}, \beta) = \beta \sum_{n=1}^{N} \{t_n - \textbf{w}^T \phi(\textbf{x}_n)\}\phi(\textbf{x}_n)^T
$$

Gradient를 $0$으로 놓으면,

$$
0 = \sum_{n=1}^{N} t_n \phi(\textbf{x}_n)^T -  \textbf{w}^T \left( \sum_{n=1}^{N} \phi(\textbf{x}_n)\phi(\textbf{x}_n)^T \right)
$$

이를 $\textbf{w}$에 풀면 다음을 얻는다.

$$
\textbf{w}_{ML} = (\Phi^T\Phi)^{-1}\Phi^T \mathsf{t}
\tag{ $ 5 $ }
$$

<br>

식 $(5)$를 least square 문제의 <span style="color:red">normal equation</span>이라고 한다. 여기서 $\Phi$는 $N \times M$ 행렬로, <span style="color:red">design matrix</span>라고 불린다. 각 원소는 $\Phi_{nj} = \phi_j(\textbf{x}_n$으로 주어진다.

$$
\Phi = 
\begin{bmatrix}
\phi_0(\textbf{x}_1) & \phi_1(\textbf{x}_1) & \cdots & \phi_{M-1}(\textbf{x}_1) \\
\phi_0(\textbf{x}_2) & \phi_1(\textbf{x}_2) & \cdots & \phi_{M-1}(\textbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(\textbf{x}_N) & \phi_1(\textbf{x}_N) & \cdots & \phi_{M-1}(\textbf{x}_N) \\
\end{bmatrix}
$$

<br>

$$
\Phi^{\dagger} \equiv (\Phi^T\Phi)^{-1}\Phi^T
\tag{ $ 6 $ }
$$

식 $(6)$을 행렬 $\Phi$의 **<span style="color:red">Moor-Penrose pseudo-inverse</span>**라고 한다. 역행렬의 개념을 정사각이 아닌 행렬로까지 일반화시킨 것.

<br>

***
### 3.1.4 Regularized Least Squares

Sec 1.1에서 over-fitting 문제를 방지하기 위해 error function에 regularization term을 추가했었다. 이를 포함한 error function은 다음 형태.

$$
E_D(\textbf{w}) + \lambda E_W(\textbf{w})
$$

$E_D$는 데이터에 종속적인 error, $E_W$는 regularization term.

가장 단순한 형태의 regularization term은 weight vector 원소들의 sum-of-square.

$$
E_W(\textbf{w}) = \frac{1}{2}\textbf{w}^T\textbf{w}
$$


다음 형태의 sum-of-square error function을 고려하자.

$$
E_D(\textbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \{\textbf{w}^T \phi(\textbf{x}_n)\}^2 + \frac{\lambda}{2}\textbf{w}^T\textbf{w}
\tag{ $ 7 $ }
$$

식 $(7)$의 $\textbf{w}$에 대한 gradient를 $0$으로 놓고 $\textbf{w}$에 대해 풀어내면

$$
\textbf{w} = (\lambda \textbf{I} + \Phi^T\Phi)^{-1}\Phi^T \mathsf{t}
$$

이는 식 $(5)$의 least square solution을 간단히 확장한 것.

<br>

좀 더 일반적인 형태의 regularization term을 사용하기도 한다. 이 경우 regularized error function은

$$
\frac{1}{2} \sum_{n=1}^{N} \{\textbf{w}^T \phi(\textbf{x}_n)\}^2 + \frac{\lambda}{2} \sum_{j=1}^{M} | w_j |^q
$$

<br>

$q=1$인 경우를 **<span style="color:red">Lasso-regularized model</span>**,

$q=2$인 경우를 **<span style="color:red">Ridge-regularized model</span>**이라고 한다.

<p align="center"><img src="/assets/image/machine_learning/prmlfigs-png/Figure3.3.png" width="" height="" title="q 값에 따른 regularization term의 윤곽선" alt=""><br/></p>

|<img src="/assets/image/machine_learning/prmlfigs-png/Figure3.4b.png" width="" height="" title="" alt=""><br/>|<img src="/assets/image/machine_learning/prmlfigs-png/Figure3.4a.png" width="" height="" title="" alt=""><br/>|

<br>

각각의 경우 parameter vector $\textbf{w}$의 최적값이 $\textbf{w}^{\ast}$로 나타나 있다.


<br>


***

<div class="footnotes"><ol>
<li class="footnote" id="fn:1">
<p>
Identically independently distributed.
<a href="#fnref:1" title=""> ↩</a><p>