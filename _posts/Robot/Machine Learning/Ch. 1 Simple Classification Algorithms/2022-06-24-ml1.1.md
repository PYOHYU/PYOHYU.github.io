---
title:  "1.1 Artificial Neuron"
excerpt: 

categories:
  - Robot
tags:
  - [Machine Learning]

toc: true
toc_sticky: true
 
date: 2022-06-24
last_modified_at: 2022-06-24

use_math: true
published: true
---

<p align="center"><img src="/assets/image/machine_learning/ch1/1.1.png" width="" height="" title="" alt=""><br/></p>

1943년, Warren McCulloch과 Walter Pits는 뇌의 뉴런으로부터 신경 시스템 모델을 제안했다. 수상 돌기에 여러 입력 신호가 도달하면 세포체에서 합쳐지고, 합쳐진 신호가 특정 임계 값을 넘으면 출력 신호가 생성되어 축삭돌기에 전달된다. McCulloch과 Pits는 신경 세포를 이진 출력을 내는 논리 회로로 표현했으며, 이를 <span style="color:red">**MCP 뉴런**</span>이라고 한다.

<br>

Frank Rosenblatt가 이를 기반으로 <span style="color:red">**perceptron**</span>을 발표한다(1957). 퍼셉트론은 인공 신경망의 하나로, 다수의 입력을 받아 하나의 결과를 내보내는 알고리즘이다.

<br>


***

### 1.1.1 Mathematical Definition

<br>

<span style="color:red">**Artificial neuron**</span>은 두 개의 클래스를 갖는 이진 분류(binary classification) 작업을 하는 회로로 생각할 수 있다. 두 클래스를 각각 $1$(positive), $-1$(negative)로 나타내자.

*Input value* $\mathbf{x}$와 *weight vector* $\mathbf{w}$의 linear combination으로 만들어지는 <span style="color:red">activation function</span>(입력 함수) $\phi(z)$을 정의한다. *Net input*(최종 입력) $z$는 $z=w_1x_1+ \cdots +w_mx_m$이다.

$$\mathbf{w} = \begin{bmatrix} w_1 \\ \vdots \\ w_m \end{bmatrix}, \qquad \mathbf{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_m \end{bmatrix}$$

<br>

잠시 데이터가 어떤 형태로 이루어져 있는지 보자.

<p align="center"><img src="/assets/image/machine_learning/ch1/1.2.png" width="" height="" title="" alt=""><br/></p>

위 그림은 머신 러닝의 주요 예제 중 하나인 **Iris dataset**(붓꽃 데이터셋)이다. Dataset은 Setosa, Versicolor, Virginica 세 종류의 붓꽃 샘플을 가지고 있다. 각각의 sample은 dataset에서 하나의 row로 표기된다. 측정값은 column에 저장되고 이를 dataset의 feature라고 한다.

<br>

이제 feature matrix $\textbf{X}$ 를 나타낼 수 있다. Dataset이 150개 sample과 4개의 feature를 가진다면 iris dataset은 matrix $\textbf{X} \in \mathbb{R}^{150 \times 4}$로 나타낼 수 있다.

$$\textbf{X} = \begin{bmatrix} x_1^{(1)} && x_2^{(1)} && x_3^{(1)} && x_4^{(1)} \\ x_1^{(2)} && x_2^{(2)} && x_3^{(2)} && x_4^{(2)} \\ \vdots && \vdots && \vdots && \vdots \\ x_1^{(150)} && x_2^{(150)} && x_3^{(150)} && x_4^{(150)}\end{bmatrix}$$

<br>

Feature matrix의 각 row는 하나의 sample을 나타낸다. 여기서는 4차원 row vector $\textbf{x}^{(i)} \in \mathbb{R}^{1 \times 4}$.

$$\textbf{x}^{(i)} = \begin{bmatrix} x_1^{(i)} && x_2^{(i)} && x_3^{(i)} && x_4^{(i)} \end{bmatrix}$$

각 feature는 150차원의 column vector $\textbf{x}_j \in \mathbb{R}^{150 \times 1}$.

$$\textbf{x}_j = \begin{bmatrix} x_j^{(1)} \\ x_j^{(2)} \\ \vdots \\ x_j^{(150)} \end{bmatrix}$$

Target variable(여기서는 class labels) $\textbf{y}$는 150차원의 column vector로 저장된다.

$$\textbf{y} = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(150)} \end{bmatrix} \qquad (y \in \{ \textrm{Setosa, Versicolor, Virginica}\})$$

<br>

다시 activation function으로 돌아가자. 특정 sample $\textbf{x}^{(i)}$의 net input이 사전에 정의된 threshold(임계 값) $\theta$보다 크면 클래스 $1$로, 그렇지 않으면 $-1$로 예측한다고 하자.

Perceptron 알고리즘에서 이 activation function은 <span style="color:red">**unit step function**</span>을 변형한 형태가 된다.

$$\phi(z) = \begin{cases} 1 & z \geq \theta \\ -1 & \textrm{otherwise} \end{cases}$$

식을 간단히 하기 위해 threshold $\theta$를 좌변으로 옮기자. $w_0 = -\theta$이고 $z_0=1$인 $0$번째 가중치를 정의. 그러면 $z$의 형태가 단순해진다.

$$z = w_0x_0 + w_1x_1+ \cdots +w_mx_m = \textbf{w}^T \textbf{x}$$

이제 activation function은

$$\phi(z) = \begin{cases} 1 & z \geq 0 \\ -1 & \textrm{otherwise} \end{cases}$$

이때 음수 threshold 또는 weight $w_0 = -\theta$를 <span style="color:red">**intercept**</span>(절편)이라고 한다.

<br>

그럼 이 perceptron activation function으로 뭘 할 수 있을까? Net input $z = \textbf{w}^T \textbf{x}$가 binary output($-1$ 또는 $1$)으로 압축되면서 선형으로 구분 가능한 두 클래스 사이를 구별할 수 있게 된다.

