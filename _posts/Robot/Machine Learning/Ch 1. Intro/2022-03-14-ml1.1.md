---
title:  "1.1 Polynomial Curve Fitting"
excerpt: 

categories:
  - Robot
tags:
  - [Machine Learning]

toc: false
toc_sticky: false
 
date: 2022-03-14
last_modified_at: 2022-03-14

use_math: true
published: true
---

<br>

***

$N$개의 관찰값 $x$로 이루어진 <span style="color:red">training set</span> $\textbf{x} \equiv (x_1, \cdots, x_N)^T$와 이에 해당하는 target value $\textbf{t} \equiv (t_1, \cdots, t_N)^T$가 주어졌다고 하자. 예를 들어 다음 그림은 $N=10$일 때의 figure.

<p align="center"><img src="/assets/image/machine_learning/prmlfigs-png/Figure1.2.png" width="50%" height="50%" title="" alt=""><br/></p>

여기서 input data set $\textbf{x}$는 균등한 거리의 $x_n$를 선택해 만들었고, target data set $\textbf{t}$의 값 $t_n$들은 함수 $\sin(2 \pi x)$의 출력값에 노이즈가 더해져서 만들어졌다.

<br>

우리 목표는 이 training set을 이용해 어떤 새로운 input $\hat{x}$이 주어졌을 때 target variable $\hat{t}$를 예측하는 것. 여기에는 기저에 있는 함수 $\sin(2 \pi x)$를 찾아내는 것 역시 포함된다.

이 한정된 dataset에서 일반화를 시킨다고? 게다가 노이즈가 있어서 주어진 $\hat{x}$에 대해 어떤 $\hat{t}$가 적합할지도 모르는데?<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>

<br>

***
### 1.1.1 Polynomial Fitting

그러니 지금은 간단히 다항식을 이용. Curve fitting을 위해,

$$
y(x, \textbf{w}) = w_0 + w_1x + w_2x^2 + \dots + w_Mx^M = \sum_{j=0}^{M} w_jx^j
$$

여기서 $\textbf{w} = (w_0, w_1, \dots w_M)^T$. Polynomial $y(x, \textbf{w})$는 $x$에 대해서는 nonlinear이나 $\textbf{w}$에 대해서는 linear.

<br>

Training set의 target value와 함숫값 $y(x, \textbf{w})$ 사이의 오차를 측정하는 **<span style="color:red">error function</span>**을 정의. 이 함수의 값을 최소화하는 방식으로 fitting을 진행할 것이다.

가장 널리 쓰이는 error function 중 하나는

$$
E(\textbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \{y(x_n, \textbf{w}) - t_n\}^2 
\tag{ $ 1 $ }
$$

각각의 data $x_n$에 대해 예측값 $y(x_n, \textbf{w})$과 해당 target value $t_n$ 사이의 오차를 제곱하여 합산한 것. 분수 $\frac{1}{2}$는 나중의 계산을 위해 편의상 도입. 이 함수의 결과값은 양수이며, 오직 $y(x, \textbf{w})$가 정확히 data point를 지날 때에만 0이 된다.

<p align="center"><img src="/assets/image/machine_learning/prmlfigs-png/Figure1.3.png" width="50%" height="50%" title="" alt=""><br/></p>

<br>

$E(\textbf{w})$를 최소화하는 $\textbf{w}$를 찾을 수 있다면 curve fitting 문제는 해결. Error function은 2차 다항식 형태이므로, 이 함수를 계수에 대해 미분하면 $\textbf{w}$에 대한 linear function이 되겠다. 그러므로 error function을 최소화하는 유일한 $\textbf{w}^{\ast}$를 찾을 수 있다.

결과에 해당하는 polynomial은 $y(x, \textbf{w}^{\ast})$ 형태일 것.

<br>

***
### 1.1.2 Over-Fitting

다음 네 가지 예시를 보자.


|<img src="/assets/image/machine_learning/prmlfigs-png/Figure1.4a.png" width="70%" height="70%" title="" alt=""><br/>|<img src="/assets/image/machine_learning/prmlfigs-png/Figure1.4b.png" width="70%" height="70%" title="" alt=""><br/>|
|<img src="/assets/image/machine_learning/prmlfigs-png/Figure1.4c.png" width="70%" height="70%" title="" alt=""><br/>|<img src="/assets/image/machine_learning/prmlfigs-png/Figure1.4d.png" width="70%" height="70%" title="" alt=""><br/>|

<br>

상수($M=0$)나 일차($M=1$) 다항식의 경우 fitting이 잘 되지 않는다. $M=3$일 때 가장 $\sin(2 \pi x)$에 가깝다.

차수가 더 높아질 경우 ($M=9$) training set에 대해 완벽한 fitting이 가능하게 된다. 이때 다항식은 모든 data point를 지나가고 $E(\textbf{w}^{\ast})=0$. 그러나 fitting된 곡선은 심하게 진동해서 오히려 $\sin(2 \pi x)$ 표현에 실패한다. 이것이 **<span style="color:red">over-fitting</span>**.

<br>

우리가 원하는 건 새로운 data에 대해 정확한 결과값 예측을 위한 좋은 일반화를 달성하는 것. $M$의 값에 따라 일반화의 성능이 어떻게 변하는지 정량적으로 알아보고 싶다.

training set과 test data set 각각에 대해 식 $(1)$로 주어진 $E(\textbf{w}^{\ast})$의 residual value를 계산하자. 자주 쓰는 것은 **<span style="color:red">root mean square</span>**.

$$
E_{\textrm{RMS}} = \sqrt{2 E(\textbf{w}^{\ast}) / N}
$$

$N$으로 나누면 data size가 달라도 비교가 가능. 제곱근으로 $E_{\textrm{RMS}}$가 target value $t$와 같은 단위를 갖도록.

<p align="center"><img src="/assets/image/machine_learning/prmlfigs-png/Figure1.5.png" width="50%" height="50%" title="" alt=""><br/></p>

$M$ 값이 $3 \leq M \leq 8$ 밤위에 있을 때 test data set의 오차가 작고 fitting된 다항식이 $\sin(2 \pi x)$를 잘 표현한다.

<br>

$M$에 따른 fitting function의 계수(벡터 $\textbf{w}^{\ast}$의 성분들)를 적은 표. $M$이 커지면 계수가 급격히 커진다.

||$M=0$|$M=1$|$M=3$|$M=9$|
|$w_0^{\ast}$|0.19|0.82|0.31|0.35|
|$w_1^{\ast}$||-1.27|7.99|232.37|
|$w_2^{\ast}$|||-25.43|-5321.83|
|$w_3^{\ast}$|||17.37|48568.31|
|$w_4^{\ast}$||||-231639.30|
|$w_5^{\ast}$||||640042.26|
|$w_6^{\ast}$||||-1061800.52|
|$w_7^{\ast}$||||1042400.18|
|$w_8^{\ast}$||||-557682.99|
|$w_9^{\ast}$||||125201.43|


<br>

### 1.1.3 Regularization

Over-fitting 문제를 해결하기 위해 자주 사용되는 기법 중 하나. 식 $(1)$ error function의 계수의 크기가 커지는 것을 막기 위해 페널티 항을 추가하는 것.

가장 단순한 형태는 각 계수들을 제곱해 합하는 것이다.

$$
\tilde{E}(\textbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \{y(x_n, \textbf{w}) - t_n\}^2 + \frac{\lambda}{2} \Vert \textbf{w} \Vert^2
\tag{ $ 2 $ }
$$

여기서 $\Vert \textbf{w} \Vert^2 \equiv \textbf{w}\textbf{w}^T \equiv w_0^2 + w_1^2 + \cdots + w_M^2$이고, $\lambda$는 error term의 중요도를 결정.

<br>

***

<div class="footnotes"><ol>
<li class="footnote" id="fn:1">
<p>
확률론으로 이러한 불확실성을 정량적으로 표현 가능.
<a href="#fnref:1" title=""> ↩</a><p>