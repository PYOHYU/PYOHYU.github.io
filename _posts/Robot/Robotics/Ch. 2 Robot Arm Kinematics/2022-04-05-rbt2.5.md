---
title:  "2.5 Numerical Solutions"
excerpt: 

categories:
  - Robot
tags:
  - [Robotics]

toc: true
toc_sticky: true
 
date: 2022-04-05
last_modified_at: 2022-04-05

use_math: true
published: true
---

<br>

***
 
### 2.4.1 Numerical Solutions

<br>

$\textbf{x} = \textbf{f}(\boldsymbol{\theta})$를 만족하는 closed form solution $\boldsymbol{\theta}$가 존재하지 않으면?

근사적으로 구할 수밖에.

<br>

***

### 2.4.2 Newton Method

<br>

<span style="color:red">**Jacobian Matrix**</span> $\textbf{J}(\boldsymbol{\theta}) = \frac{\partial \textbf{x}}{\partial \boldsymbol{\theta}}$를 사용한다.

만약 2 DOF manipulator라면,

$$
x = a_1c_1 + a_2c_{12} \\
y = a_1s_1 + a_2s_{12}
$$

$$
\textbf{J}(\boldsymbol{\theta}) = \begin{bmatrix}
\frac{\partial x}{\partial \theta_1} & \frac{\partial x}{\partial \theta_2} \\
\frac{\partial y}{\partial \theta_1} & \frac{\partial y}{\partial \theta_2}
\end{bmatrix} = \begin{bmatrix}
-(a_1s_1+a_2s_{12}) & -a_2s_{12} \\
a_1c_1 + a_2c_{12} & a_2c_{12}
\end{bmatrix}
$$

<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>

Taylor approximation.

$$
\textbf{x} = \textbf{f}(\boldsymbol{\theta}) = \textbf{f}(\boldsymbol{\theta}^k) + \textbf{J}(\boldsymbol{\theta}^k)(\boldsymbol{\theta}-\boldsymbol{\theta}^k) + O(\Vert \boldsymbol{\theta}-\boldsymbol{\theta}^k \Vert^2)
$$

뒤쪽 2차 이상의 항은 필요없다.

$$
\boldsymbol{\theta}^{k+1} = \boldsymbol{\theta}^k + [\textbf{J}(\boldsymbol{\theta}^k)]^{-1}[\textbf{x} - \textbf{f}(\boldsymbol{\theta}^k)]
$$

이 식이 converge하기 위해서는 initial guess $\boldsymbol{\theta}$가 $\textbf{x} = \textbf{f}(\boldsymbol{\theta})$의 실제 해와 충분히 가까워야 한다.

<br>

문제는 Jacobian matrix의 singularity 주변에서 발생. Singular case에서는 inverse $\textbf{J}^{-1}$가 존재하지 않는다. 위 식 자체를 써먹을 수가 없다는 것.

<br>

### 2.4.3 Gradient Method
<br>
<span style="color:red">**Gradient method**</span>는 error function을 최소화시키는 방식으로 해를 구한다. Error function의 종류는 여러가지 있겠지만 자주 쓰는 것은 바로 다음 형태.

$$
H(\boldsymbol{\theta}) = \frac{1}{2}\Vert \textbf{x}-\textbf{f}(\boldsymbol{\theta}) \Vert = \frac{1}{2}[\textbf{x}-\textbf{f}(\boldsymbol{\theta})]^T[\textbf{x}-\textbf{f}(\boldsymbol{\theta})]
$$

이때, error function의 gradient가

$$
\nabla_q H(\boldsymbol{\theta}) = -\textbf{J}^T(\boldsymbol{\theta})[\textbf{x} - \textbf{f}(\boldsymbol{\theta})]
$$

이므로, 다음 iteration equation을 얻는다.

$$
\boldsymbol{\theta}^{k+1} = \boldsymbol{\theta}^k + \alpha\textbf{J}^T(\boldsymbol{\theta}^k)[\textbf{x} - \textbf{f}(\boldsymbol{\theta}^k)]
$$

여기서 $\alpha$는 step size로 적절하게 결정하면 된다.

<br>

Gradient method가 계산이 더 간단하다. Newton method는 Jacobian의 inverse를 사용하지만, gradient method는 jacobian의 transpose만 있으면 된다.

따라서, converge하지 않을 수는 있어도, diverge하지는 않는다!

<br>

물론 이 해 역시 초기 guess에 의존한다. 오직 하나의 해만 얻을 수 있고, 여러 해를 얻으려면 여러 intial guess들이 필요하다.

<br>

***

### 2.4.4 Stopping Criteria

<br>

Iteration을 끝내고 해를 얻기 위해 최소 오차를 설정할 필요가 있다.

$$
\Vert \textbf{x} - \textbf{f}(\boldsymbol{\theta}) \Vert \leq \epsilon , \qquad \Vert \boldsymbol{\theta}^{k+1} - \boldsymbol{\theta}^k \Vert \leq \epsilon
$$


<div class="footnotes"><ol>
<li class="footnote" id="fn:1">
<p>
Vector를 vector로 미분한 변화율.
<a href="#fnref:1" title=""> ↩</a><p>